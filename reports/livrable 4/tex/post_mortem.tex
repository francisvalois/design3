%!TEX root = ../rapport.tex
%!TEX encoding = UTF-8 Unicode

% Chapitres "Introduction"

% modifié par Francis Valois, Université Laval
% 31/01/2011 - version 1.0 - Création du document
\chapter{Post-mortem}

\section{Vision}

\subsection{Orientation avec la caméra embarquée}

L'utilisation de la matrice de transformation pour la localisation exige une matrice extrinsèque la plus parfaite possible. Avec plus de temps, il aurait peut-être été possible d'obtenir une bonne matrice et de la tester adéquatement. Il existe aussi une fonction dans opencv, reprojectImageTo3D pour la localisation des objets dans un espace 3D avec deux images prises avec deux caméras ou deux angles différents qui auraient pu être utilisées avec plus de temps et une calibration adéquate. 

Une autre méthode qui aurait pu être envisagée est une look-up table constituée en prenant une image d'une grille avec correspondance entre des points dont les coordonnées par rapport au robot sont connues. La transformation de la position du pixel dans l'image vers leurs coordonnées par rapport au robot aurait été faite en localisant les valeurs auxquelles elles correspondent dans la table et en effectuant une interpolation linéaire. Cette méthode aurait eu l'avantage d'être plus facilement et plus intuitivement testable que celles utilisant des matrices.

La localisation avec la caméra embarquée aurait potentiellement été plus précise que celle avec la Kinect, et particulièrement d'obtenir la position du robot lorsqu'il n'est pas possible de le faire avec la Kinect, ce qui survient surtout dans les positions les plus éloignées. 

La fiabilité de la méthode de localisation des points à l'intersection des Hough lines aurait également dû être améliorée pour pouvoir l'utiliser dans un algorithme de localisation. En ajustant la saturation des images prises et la segmentation, il était possible d'avoir la position des coins inférieurs des blocs de couleur dans la plupart des cas, mais il arrivait souvent que le positionnement s'écarte beaucoup de la position réelle à cause de l'erreur sur les droites retenues.


\subsection{Vision par Kinect}
La vision à l'aide de la Kinect a été une des parties les plus ardues tout au long du projet. Sachant que la technologie est relativement nouvelle, la documentation à son sujet est très restreinte et souvent incomplète. Il a donc fallu effectuer plusieurs tests et développer nos propres algorithmes pour arriver à quelque chose de fonctionnel. De plus, les frameworks ne semblent pas encore tout à fait matures pour en faire une utilisation simple. Nous avons qu'à penser à la distorsion de la caméra infrarouge qui a causé d'énorme problème lors des tests de nos algorithmes de transformation de distances. Cette distorsion pourrait être simplement corrigée au travers du framework et la documentation pourrait en faire mention. Une autre idée possible pour améliorer la précision est l'utilisation d'une lookup table de correction en XZ. En effectuant la même méthodologie que pour la courbe de correction en X présenté à la section \ref{s:distortion}, mais à plus grande échelle, soit plus de 50 points partout sur la table, il serait possible d'obtenir une très bonne correction.

Toutefois, outre le problème majeur de distorsion, la Kinect permet d'obtenir facilement la distance de tout objet dans son champ de vision avec une certaine précision. Ainsi, à l'aide d'algorithme de détection et de statistique, il a été fort simple d'obtenir la position en 3D des obstacles et du robot sur la table. Comme la Kinect effectue tout le traitement de conversion IR-Distance, nos algorithmes sont très efficaces en temps d'exécution. Ainsi, pour la détection des obstacles, l'algorithme prend entre 30 et 60 ms ce qui est très rapide pour une telle détection. Du côté de la détection du robot, il a été nécessaire d'utiliser la caméra RGB en plus de la caméra infrarouge, car le robot est presque aussi haut que les murs de la table. Ainsi, il devient ardu d'extraire le robot du nuage de point lorsque celui-ci est proche d'un mur ou bien des obstacles. La principale difficulté de l'utilisation de la caméra RGB est de savoir ce que nous devons ignorer et ce que nous devons garder. Dans la première version de la détection du cadre bleu sur le robot, dès que quelque chose de bleu entrait dans le champ de vision de la caméra, l'algorithme n'était plus apte à trouver la position du robot et retournait des valeurs aberrantes. Dans la deuxième version, chacune des zones bleues trouvées par l'algorithme est comparée aux distances de la Kinect. Ainsi, lorsque les zones bleues sont situées à l'extérieur de la table, il suffit de les ignorer. Cette composition IR-RGB est vraiment intéressante. Toutefois, en se servant de la caméra RGB, il faut ajouter un temps considérable de traitement pour trouver le cadre bleu ainsi que les différents petits carrés, car la détection n'est pas effectuée directement par la Kinect comme la conversion IR-Distance. Ce traitement de l'image ajoute environ 200ms de plus aux 30 à 60ms de détection de forme dans le nuage de point. Ce temps d'excution est toutefois en déca de la seconde et est donc extremement rapide et adapté à la mise à jour en temps réel de la position du robot.

En écartant la précision imparfaite de notre implantation qui ne demande qu'à être améliorée, le placement des cadres bleus oblige le robot à se placer d'une certaine façon pour que celui-ci soit détecté. Une amélioration possible aurait été de placer des carrés sur les faces non détectées pour permettre la détection à 360 degrés, mais il aurait été nécessaire de créer un algorithme de choix de carrés pour en choisir 1 seul, car dans la plupart des cas, deux carrés auraient été visibles.

\section{Déplacements}

Dans l'ensemble, le robot a un asservissement linéaire en vitesse qui dépasse largement les autres équipes en termes de temps de réponse et de stabilité directionnelle. La stratégie de déplacement aurait pu être optimisée avec l'utilisation des déplacements en diagonal. Comme l'asservissement était en vitesse, nous aurions eu à intégrer des plages de décélérations calibrées selon les plages d'asservissements. Afin d'assurer l'unicité de l'ensemble, d'autres calibres d'asservissements auraient dû être ajoutés. 

\section{Dessins}

Afin de compenser les disparités de constitutions des roues de notre robot, qui avaient énormément d'impact sur nos performances, un asservissement en position utilisant la vision aurait pu être implanté afin de compenser les erreurs. Pour se faire, nous aurions pu centrer le préhenseur et effectuer des rotations lors des croisements afin de toujours détecter le cadre vert. L'implantation de l'asservissement de déplacement pour les diagonales aurait pu être employé à cet effet, jumelé à la détection du cadre vert, qui aurait du être adaptée pour un robot qui se déplace. Évidemment, le temps requis de conception est particulièrement énorme et un tel ajustement aurait requis plusieurs semaines. 

