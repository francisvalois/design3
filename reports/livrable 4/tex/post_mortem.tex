%!TEX root = ../rapport.tex
%!TEX encoding = UTF-8 Unicode

% Chapitres "Introduction"

% modifié par Francis Valois, Université Laval
% 31/01/2011 - version 1.0 - Création du document
\chapter{Post-mortem}

\section{Vision}
\subsection{Vision par Kinect}
La vision à l'aide de la Kinect a été une des parties les plus ardues tout au long du projet. Sachant que la technologie est relativement nouvelle, la documentation à son sujet est très restreinte et souvent incomplète. Il a donc fallu effectuer plusieurs tests et développer nos propres algorithmes pour arriver à quelque chose de fonctionnel. De plus, les frameworks ne semblent pas encore tout à fait matures pour en faire une utilisation simple. Nous avons qu'à penser à la distorsion de la caméra infrarouge qui a causé d'énorme problème lors des tests de nos algorithmes de transformation de distances. Cette distorsion pourrait être simplement corrigée au travers du framework et la documentation pourrait en faire mention.

Toutefois, outre le problème majeur de distorsion, la Kinect permet d'obtenir facilement la distance de tout objet dans son champ de vision avec une certaine précision. Ainsi, à l'aide d'algorithme de détection et de statistique, il a été fort simple d'obtenir la position en 3D des obstacles et du robot sur la table. Comme la Kinect effectue tout le traitement de conversion IR-Distance, nos algorithmes sont très efficaces en temps d'exécution. Ainsi, pour la détection des obstacles, l'algorithme prend entre 30 et 60 ms ce qui est très rapide pour une telle détection. Du côté de la détection du robot, il a été nécessaire d'utiliser la caméra RGB en plus de la caméra infrarouge, car le robot est presque aussi haut que les murs de la table. Ainsi, il devient ardu d'extraire le robot du nuage de point lorsque celui-ci est proche d'un mur ou bien des obstacles. La principale difficulté de l'utilisation de la caméra RGB est de savoir ce que nous devons ignorer et ce que nous devons garder. Dans la première version de la détection du cadre bleu sur le robot, dès que quelque chose de bleu entrait dans le champ de vision de la caméra, l'algorithme n'était plus apte à trouver la position du robot et retournait des valeurs aberrantes. Dans la deuxième version, chacune des zones bleues trouvées par l'algorithme est comparée aux distances de la Kinect. Ainsi, lorsque les zones bleues sont situées à l'extérieur de la table, il suffit de les ignorer. Cette composition IR-RGB est vraiment intéressante. Toutefois, en se servant de la caméra RGB, il faut ajouter un temps considérable de traitement pour trouver le cadre bleu ainsi que les différents petits carrés, car la détection n'est pas effectuée directement par la Kinect comme la conversion IR-Distance. Ce traitement de l'image ajoute environ 200ms de plus aux 30 à 60ms de détection de forme dans le nuage de point.

En écartant la précision imparfaite de notre implantation qui ne demande qu'à être améliorée, le placement des cadres bleus oblige le robot à se placer d'une certaine façon pour que celui-ci soit détecté. Une amélioration possible aurait été de placer des carrés sur les faces non détectées pour permettre la détection à 360 degrés, mais il aurait été nécessaire de créer un algorithme de choix de carrés pour en choisir 1 seul, car dans la plupart des cas, deux carrés auraient été visibles.

